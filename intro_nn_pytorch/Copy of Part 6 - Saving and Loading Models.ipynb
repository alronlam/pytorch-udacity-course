{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Part 6 - Saving and Loading Models.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/Part%206%20-%20Saving%20and%20Loading%20Models.ipynb","timestamp":1543474349899}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"0Q0f8g6xPR0S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":326},"outputId":"5bb0a8f1-7c1f-4eb6-fe39-8c753f172aa2","executionInfo":{"status":"ok","timestamp":1543474517296,"user_tz":-480,"elapsed":103703,"user":{"displayName":"Alron Jan Lam","photoUrl":"","userId":"04554281822716608040"}}},"cell_type":"code","source":["!pip3 install torch torchvision"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting torch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n","\u001b[K    100% |████████████████████████████████| 519.5MB 28kB/s \n","tcmalloc: large alloc 1073750016 bytes == 0x58e46000 @  0x7f3d10fd12a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n","\u001b[?25hCollecting torchvision\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 23.5MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Collecting pillow>=4.1.1 (from torchvision)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n","\u001b[K    100% |████████████████████████████████| 2.0MB 4.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n","Installing collected packages: torch, pillow, torchvision\n","  Found existing installation: Pillow 4.0.0\n","    Uninstalling Pillow-4.0.0:\n","      Successfully uninstalled Pillow-4.0.0\n","Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1\n"],"name":"stdout"}]},{"metadata":{"id":"KIPiGcQrQGHF","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":72},"outputId":"5d8b9ed3-43a2-474e-85dd-19a61f266dcf","executionInfo":{"status":"ok","timestamp":1543475077401,"user_tz":-480,"elapsed":9658,"user":{"displayName":"Alron Jan Lam","photoUrl":"","userId":"04554281822716608040"}}},"cell_type":"code","source":["# File Uploader\n","from google.colab import files\n","src = list(files.upload().values())[0]\n","open('mylib.py','wb').write(src)\n","import mylib"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-b8b72294-9bea-4779-ab8b-2fb650300c3b\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-b8b72294-9bea-4779-ab8b-2fb650300c3b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving fc_model.py to fc_model.py\n"],"name":"stdout"}]},{"metadata":{"id":"5WisvPHjPKyL","colab_type":"text"},"cell_type":"markdown","source":["# Saving and Loading Models\n","\n","In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."]},{"metadata":{"id":"bxP6GMl7PKyQ","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","\n","import helper\n","import fc_model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f6C6N9oGPKyW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"003e2cb2-20a3-402d-9d5e-e26a78b5a90d","executionInfo":{"status":"ok","timestamp":1543475167238,"user_tz":-480,"elapsed":5014,"user":{"displayName":"Alron Jan Lam","photoUrl":"","userId":"04554281822716608040"}}},"cell_type":"code","source":["# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.5,), (0.5,))])\n","# Download and load the training data\n","trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n","\n","# Download and load the test data\n","testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"metadata":{"id":"LQOUpBSrPKyb","colab_type":"text"},"cell_type":"markdown","source":["Here we can see one of the images."]},{"metadata":{"id":"FnJzvdOYPKyb","colab_type":"code","outputId":"a6cf2ecc-fcf2-4221-9c27-f0258914ef50","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"ok","timestamp":1543475593957,"user_tz":-480,"elapsed":954,"user":{"displayName":"Alron Jan Lam","photoUrl":"","userId":"04554281822716608040"}}},"cell_type":"code","source":["image, label = next(iter(trainloader))\n","helper.imshow(image[0,:]);"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAc8AAAHPCAYAAAA1eFErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f1c34b108d0>"]},"metadata":{"tags":[],"image/png":{"width":231,"height":231}}}]},{"metadata":{"id":"IQFZF0MFPKyh","colab_type":"text"},"cell_type":"markdown","source":["# Train a network\n","\n","To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to demonstrate how we can save and load models."]},{"metadata":{"id":"-4kDVXY3PKyi","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create the network, define the criterion and optimizer\n","\n","model = fc_model.Network(784, 10, [512, 256, 128])\n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"56QRAZBvPKyn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":799},"outputId":"b178e0d8-cae9-4e8c-bcf4-3156ec101b3a","executionInfo":{"status":"ok","timestamp":1543475832751,"user_tz":-480,"elapsed":97979,"user":{"displayName":"Alron Jan Lam","photoUrl":"","userId":"04554281822716608040"}}},"cell_type":"code","source":["fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch: 1/2..  Training Loss: 1.681..  Test Loss: 0.979..  Test Accuracy: 0.589\n","Epoch: 1/2..  Training Loss: 1.060..  Test Loss: 0.743..  Test Accuracy: 0.735\n","Epoch: 1/2..  Training Loss: 0.838..  Test Loss: 0.686..  Test Accuracy: 0.732\n","Epoch: 1/2..  Training Loss: 0.801..  Test Loss: 0.639..  Test Accuracy: 0.752\n","Epoch: 1/2..  Training Loss: 0.743..  Test Loss: 0.608..  Test Accuracy: 0.767\n","Epoch: 1/2..  Training Loss: 0.729..  Test Loss: 0.588..  Test Accuracy: 0.773\n","Epoch: 1/2..  Training Loss: 0.680..  Test Loss: 0.588..  Test Accuracy: 0.769\n","Epoch: 1/2..  Training Loss: 0.684..  Test Loss: 0.574..  Test Accuracy: 0.789\n","Epoch: 1/2..  Training Loss: 0.663..  Test Loss: 0.556..  Test Accuracy: 0.796\n","Epoch: 1/2..  Training Loss: 0.640..  Test Loss: 0.561..  Test Accuracy: 0.790\n","Epoch: 1/2..  Training Loss: 0.633..  Test Loss: 0.533..  Test Accuracy: 0.806\n","Epoch: 1/2..  Training Loss: 0.628..  Test Loss: 0.546..  Test Accuracy: 0.801\n","Epoch: 1/2..  Training Loss: 0.641..  Test Loss: 0.529..  Test Accuracy: 0.808\n","Epoch: 1/2..  Training Loss: 0.612..  Test Loss: 0.516..  Test Accuracy: 0.810\n","Epoch: 1/2..  Training Loss: 0.570..  Test Loss: 0.518..  Test Accuracy: 0.807\n","Epoch: 1/2..  Training Loss: 0.631..  Test Loss: 0.501..  Test Accuracy: 0.812\n","Epoch: 1/2..  Training Loss: 0.572..  Test Loss: 0.523..  Test Accuracy: 0.811\n","Epoch: 1/2..  Training Loss: 0.593..  Test Loss: 0.503..  Test Accuracy: 0.820\n","Epoch: 1/2..  Training Loss: 0.598..  Test Loss: 0.497..  Test Accuracy: 0.819\n","Epoch: 1/2..  Training Loss: 0.600..  Test Loss: 0.508..  Test Accuracy: 0.815\n","Epoch: 1/2..  Training Loss: 0.583..  Test Loss: 0.504..  Test Accuracy: 0.818\n","Epoch: 1/2..  Training Loss: 0.559..  Test Loss: 0.483..  Test Accuracy: 0.821\n","Epoch: 1/2..  Training Loss: 0.576..  Test Loss: 0.502..  Test Accuracy: 0.821\n","Epoch: 2/2..  Training Loss: 0.551..  Test Loss: 0.482..  Test Accuracy: 0.825\n","Epoch: 2/2..  Training Loss: 0.579..  Test Loss: 0.473..  Test Accuracy: 0.828\n","Epoch: 2/2..  Training Loss: 0.544..  Test Loss: 0.479..  Test Accuracy: 0.824\n","Epoch: 2/2..  Training Loss: 0.543..  Test Loss: 0.477..  Test Accuracy: 0.830\n","Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.461..  Test Accuracy: 0.834\n","Epoch: 2/2..  Training Loss: 0.581..  Test Loss: 0.481..  Test Accuracy: 0.827\n","Epoch: 2/2..  Training Loss: 0.540..  Test Loss: 0.470..  Test Accuracy: 0.832\n","Epoch: 2/2..  Training Loss: 0.525..  Test Loss: 0.467..  Test Accuracy: 0.834\n","Epoch: 2/2..  Training Loss: 0.538..  Test Loss: 0.455..  Test Accuracy: 0.834\n","Epoch: 2/2..  Training Loss: 0.526..  Test Loss: 0.466..  Test Accuracy: 0.829\n","Epoch: 2/2..  Training Loss: 0.514..  Test Loss: 0.473..  Test Accuracy: 0.825\n","Epoch: 2/2..  Training Loss: 0.541..  Test Loss: 0.472..  Test Accuracy: 0.829\n","Epoch: 2/2..  Training Loss: 0.542..  Test Loss: 0.470..  Test Accuracy: 0.827\n","Epoch: 2/2..  Training Loss: 0.559..  Test Loss: 0.463..  Test Accuracy: 0.832\n","Epoch: 2/2..  Training Loss: 0.510..  Test Loss: 0.451..  Test Accuracy: 0.838\n","Epoch: 2/2..  Training Loss: 0.545..  Test Loss: 0.451..  Test Accuracy: 0.833\n","Epoch: 2/2..  Training Loss: 0.502..  Test Loss: 0.441..  Test Accuracy: 0.839\n","Epoch: 2/2..  Training Loss: 0.533..  Test Loss: 0.450..  Test Accuracy: 0.834\n","Epoch: 2/2..  Training Loss: 0.542..  Test Loss: 0.445..  Test Accuracy: 0.836\n","Epoch: 2/2..  Training Loss: 0.552..  Test Loss: 0.475..  Test Accuracy: 0.827\n","Epoch: 2/2..  Training Loss: 0.519..  Test Loss: 0.454..  Test Accuracy: 0.833\n","Epoch: 2/2..  Training Loss: 0.517..  Test Loss: 0.446..  Test Accuracy: 0.838\n","Epoch: 2/2..  Training Loss: 0.508..  Test Loss: 0.433..  Test Accuracy: 0.842\n"],"name":"stdout"}]},{"metadata":{"id":"cGIsaqhPPKyq","colab_type":"text"},"cell_type":"markdown","source":["## Saving and loading networks\n","\n","As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n","\n","The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."]},{"metadata":{"id":"9xsUjrX8PKyr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":292},"outputId":"0ac02f61-8bd4-489a-e40e-154597364ead","executionInfo":{"status":"ok","timestamp":1543475891429,"user_tz":-480,"elapsed":815,"user":{"displayName":"Alron Jan Lam","photoUrl":"","userId":"04554281822716608040"}}},"cell_type":"code","source":["print(\"Our model: \\n\\n\", model, '\\n')\n","print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Our model: \n","\n"," Network(\n","  (hidden_layers): ModuleList(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): Linear(in_features=512, out_features=256, bias=True)\n","    (2): Linear(in_features=256, out_features=128, bias=True)\n","  )\n","  (output): Linear(in_features=128, out_features=10, bias=True)\n","  (dropout): Dropout(p=0.5)\n",") \n","\n","The state dict keys: \n","\n"," odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"],"name":"stdout"}]},{"metadata":{"id":"fMGacAQVPKyt","colab_type":"text"},"cell_type":"markdown","source":["The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."]},{"metadata":{"id":"eHOiyF_QPKyv","colab_type":"code","colab":{}},"cell_type":"code","source":["torch.save(model.state_dict(), 'checkpoint.pth')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VK5bho9BPKyz","colab_type":"text"},"cell_type":"markdown","source":["Then we can load the state dict with `torch.load`."]},{"metadata":{"id":"9wlA8WRWPKy0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"85e0feb8-0d91-4a4b-b9bc-873ae1b60300","executionInfo":{"status":"ok","timestamp":1543476768621,"user_tz":-480,"elapsed":820,"user":{"displayName":"Alron Jan Lam","photoUrl":"","userId":"04554281822716608040"}}},"cell_type":"code","source":["state_dict = torch.load('checkpoint.pth')\n","print(state_dict.keys())"],"execution_count":13,"outputs":[{"output_type":"stream","text":["odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"],"name":"stdout"}]},{"metadata":{"id":"Cn0nXhypPKy3","colab_type":"text"},"cell_type":"markdown","source":["And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."]},{"metadata":{"id":"ViipAj2nPKy4","colab_type":"code","colab":{}},"cell_type":"code","source":["model.load_state_dict(state_dict)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gTzPYocgPKy7","colab_type":"text"},"cell_type":"markdown","source":["Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."]},{"metadata":{"id":"8vE_T3dfPKy8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":455},"outputId":"b2a4d30a-b9a3-4af5-d735-6ed916da8c2d","executionInfo":{"status":"error","timestamp":1543476817858,"user_tz":-480,"elapsed":819,"user":{"displayName":"Alron Jan Lam","photoUrl":"","userId":"04554281822716608040"}}},"cell_type":"code","source":["# Try this\n","model = fc_model.Network(784, 10, [400, 200, 100])\n","# This will throw an error because the tensor sizes are wrong!\n","model.load_state_dict(state_dict)"],"execution_count":15,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-cc11e1013989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 719\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param of torch.Size([400, 784]) from checkpoint, where the shape is torch.Size([512, 784]) in current model.\n\tsize mismatch for hidden_layers.0.bias: copying a param of torch.Size([400]) from checkpoint, where the shape is torch.Size([512]) in current model.\n\tsize mismatch for hidden_layers.1.weight: copying a param of torch.Size([200, 400]) from checkpoint, where the shape is torch.Size([256, 512]) in current model.\n\tsize mismatch for hidden_layers.1.bias: copying a param of torch.Size([200]) from checkpoint, where the shape is torch.Size([256]) in current model.\n\tsize mismatch for hidden_layers.2.weight: copying a param of torch.Size([100, 200]) from checkpoint, where the shape is torch.Size([128, 256]) in current model.\n\tsize mismatch for hidden_layers.2.bias: copying a param of torch.Size([100]) from checkpoint, where the shape is torch.Size([128]) in current model.\n\tsize mismatch for output.weight: copying a param of torch.Size([10, 100]) from checkpoint, where the shape is torch.Size([10, 128]) in current model."]}]},{"metadata":{"id":"z5iyup6-PKy9","colab_type":"text"},"cell_type":"markdown","source":["This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model."]},{"metadata":{"id":"QrFvju94PKy-","colab_type":"code","colab":{}},"cell_type":"code","source":["checkpoint = {'input_size': 784,\n","              'output_size': 10,\n","              'hidden_layers': [each.out_features for each in model.hidden_layers],\n","              'state_dict': model.state_dict()}\n","\n","torch.save(checkpoint, 'checkpoint.pth')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Br8DaRByPKzA","colab_type":"text"},"cell_type":"markdown","source":["Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "]},{"metadata":{"id":"eGtGcpc7PKzB","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_checkpoint(filepath):\n","    checkpoint = torch.load(filepath)\n","    model = fc_model.Network(checkpoint['input_size'],\n","                             checkpoint['output_size'],\n","                             checkpoint['hidden_layers'])\n","    model.load_state_dict(checkpoint['state_dict'])\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9KY3uEhYPKzD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"6842b94d-57fb-49ea-fd14-6e47b8faabf2","executionInfo":{"status":"ok","timestamp":1543477036476,"user_tz":-480,"elapsed":795,"user":{"displayName":"Alron Jan Lam","photoUrl":"","userId":"04554281822716608040"}}},"cell_type":"code","source":["model = load_checkpoint('checkpoint.pth')\n","print(model)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Network(\n","  (hidden_layers): ModuleList(\n","    (0): Linear(in_features=784, out_features=400, bias=True)\n","    (1): Linear(in_features=400, out_features=200, bias=True)\n","    (2): Linear(in_features=200, out_features=100, bias=True)\n","  )\n","  (output): Linear(in_features=100, out_features=10, bias=True)\n","  (dropout): Dropout(p=0.5)\n",")\n"],"name":"stdout"}]}]}